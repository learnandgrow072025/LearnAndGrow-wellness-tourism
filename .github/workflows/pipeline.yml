name: Tourism Project Pipeline

on:
  push:
    branches:
      - [ main ]  # Automatically triggers on push to the main branch

jobs:

  register-dataset:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas datasets huggingface_hub
      - name: Upload Dataset to Hugging Face Hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - << 'PY'
          import os
          from huggingface_hub import login, HfApi, upload_file
          login(token=os.environ["HF_TOKEN"])
          api = HfApi(token=os.environ["HF_TOKEN"])

          # dataset repo id
          DATASET_ID = "LearnAndGrow/tourism-data"

          # ensure dataset repo exists
          try:
              api.create_repo(repo_id=DATASET_ID, repo_type="dataset", private=False)
          except Exception:
              pass

          # local csv path inside repo (adjust if your csv is elsewhere)
          local_csv = "tourism_project/data/tourism.csv"

          # fail clearly if missing
          if not os.path.exists(local_csv):
              raise FileNotFoundError(f"Dataset file not found at {local_csv}. Commit it to the repo or update the path.")

          upload_file(
              path_or_fileobj=local_csv,
              path_in_repo="tourism.csv",
              repo_id=DATASET_ID,
              repo_type="dataset",
              token=os.environ["HF_TOKEN"]
          )
          print(f"✅ Uploaded dataset to https://huggingface.co/datasets/{DATASET_ID}")
          PY

  data-prep:
    needs: register-dataset
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas scikit-learn datasets huggingface_hub joblib
      - name: Run Data Preparation
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - << 'PY'
          import os
          import pandas as pd
          from datasets import load_dataset
          from huggingface_hub import login, upload_file

          login(token=os.environ["HF_TOKEN"])
          DATASET_ID = "LearnAndGrow/tourism-data"

          # Load from HF
          ds = load_dataset(DATASET_ID, data_files="tourism.csv", split="train")
          df = ds.to_pandas()

          # --- Cleaning rules from earlier chats ---
          # drop accidental index column if present
          if df.columns[0].startswith("Unnamed"):
              df = df.drop(columns=[df.columns[0]])

          # normalize Gender
          if "Gender" in df.columns:
              df["Gender"] = df["Gender"].replace({
                  "Fe Male": "Female",
                  "female": "Female",
                  "FEMALE": "Female",
                  "MALE": "Male"
              })
              df["Gender"] = df["Gender"].astype(str).str.title()

          # normalize MaritalStatus
          if "MaritalStatus" in df.columns:
              df["MaritalStatus"] = df["MaritalStatus"].replace({"Unmarried": "Single"})
              df["MaritalStatus"] = df["MaritalStatus"].astype(str).str.title()

          # ensure target is present
          assert "ProdTaken" in df.columns, "Target column 'ProdTaken' not found."

          # basic NA handling consistent with earlier guidance
          if "NumberOfChildrenVisiting" in df.columns:
              df["NumberOfChildrenVisiting"] = df["NumberOfChildrenVisiting"].fillna(0)
          df = df.dropna(subset=["ProdTaken"])
          df = df.dropna()

          # split
          from sklearn.model_selection import train_test_split
          train_df, test_df = train_test_split(
              df, test_size=0.2, random_state=42, stratify=df["ProdTaken"]
          )

          os.makedirs("data", exist_ok=True)
          train_df.to_csv("data/train.csv", index=False)
          test_df.to_csv("data/test.csv", index=False)

          # push train/test back to HF dataset
          for local, remote in [("data/train.csv", "train.csv"), ("data/test.csv", "test.csv")]:
              upload_file(
                  path_or_fileobj=local,
                  path_in_repo=remote,
                  repo_id=DATASET_ID,
                  repo_type="dataset",
                  token=os.environ["HF_TOKEN"]
              )
          print(f"✅ Pushed train/test to https://huggingface.co/datasets/{DATASET_ID}")
          PY


  model-traning:
    needs: data-prep
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas scikit-learn xgboost joblib datasets huggingface_hub mlflow
      - name: Start MLflow Server
        run: |
          nohup mlflow ui --host 0.0.0.0 --port 5000 &  # Run MLflow UI in the background
          sleep 5  # Wait for a moment to let the server starts
      - name: Model Building
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - << 'PY'
          import os, json
          import pandas as pd
          from datasets import load_dataset
          from sklearn.model_selection import GridSearchCV
          from sklearn.metrics import f1_score, accuracy_score, classification_report
          from huggingface_hub import login, HfApi, upload_file
          import joblib
          import mlflow
          import mlflow.sklearn
          from xgboost import XGBClassifier

          login(token=os.environ["HF_TOKEN"])
          DATASET_ID = "LearnAndGrow/tourism-data"
          MODEL_REPO = "LearnAndGrow/wellness-tourism-model"

          # Load train/test from HF dataset
          train_df = load_dataset(DATASET_ID, data_files="train.csv", split="train").to_pandas()
          test_df  = load_dataset(DATASET_ID, data_files="test.csv",  split="train").to_pandas()

          # X / y
          y_train = train_df["ProdTaken"]
          X_train = pd.get_dummies(train_df.drop(columns=["ProdTaken"]))
          y_test  = test_df["ProdTaken"]
          X_test  = pd.get_dummies(test_df.drop(columns=["ProdTaken"]))
          # align columns
          X_test = X_test.reindex(columns=X_train.columns, fill_value=0)

          # Model + tuning
          model = XGBClassifier(use_label_encoder=False, eval_metric="logloss", n_jobs=2)
          param_grid = {
              "n_estimators": [100, 200],
              "max_depth": [3, 5],
              "learning_rate": [0.1, 0.05]
          }
          grid = GridSearchCV(model, param_grid, cv=3, scoring="f1")
          with mlflow.start_run(run_name="xgb_gridsearch"):
              grid.fit(X_train, y_train)
              best = grid.best_estimator_
              y_pred = best.predict(X_test)
              acc = accuracy_score(y_test, y_pred)
              f1 = f1_score(y_test, y_pred)

              # log params/metrics to MLflow
              mlflow.log_params(grid.best_params_)
              mlflow.log_metric("accuracy", acc)
              mlflow.log_metric("f1", f1)

          # persist artifacts locally
          os.makedirs("artifacts", exist_ok=True)
          joblib.dump(best, "artifacts/xgb_wellness_model.pkl")
          with open("artifacts/best_params.json","w") as f: json.dump(grid.best_params_, f, indent=2)
          with open("artifacts/metrics.json","w") as f: json.dump({"accuracy":acc, "f1":f1}, f, indent=2)
          with open("artifacts/classification_report.txt","w") as f: f.write(classification_report(y_test, y_pred))

          # upload artifacts to HF Model Hub
          api = HfApi(token=os.environ["HF_TOKEN"])
          try:
              api.create_repo(repo_id=MODEL_REPO, repo_type="model", private=False)
          except Exception:
              pass

          for local, remote in [
              ("artifacts/xgb_wellness_model.pkl","xgb_wellness_model.pkl"),
              ("artifacts/best_params.json","best_params.json"),
              ("artifacts/metrics.json","metrics.json"),
              ("artifacts/classification_report.txt","classification_report.txt"),
          ]:
              upload_file(
                  path_or_fileobj=local,
                  path_in_repo=remote,
                  repo_id=MODEL_REPO,
                  repo_type="model",
                  token=os.environ["HF_TOKEN"]
              )
          print(f"✅ Model & artifacts uploaded to https://huggingface.co/{MODEL_REPO}")
          PY


  deploy-hosting:
    runs-on: ubuntu-latest
    needs: [model-traning,data-prep,register-dataset]
    steps:
      - uses: actions/checkout@v3
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install huggingface_hub
      - name: Push files to Frontend Hugging Face Space
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - << 'PY'
          import os
          from huggingface_hub import login, HfApi, upload_file
          login(token=os.environ["HF_TOKEN"])
          api = HfApi(token=os.environ["HF_TOKEN"])

          SPACE_ID = "LearnAndGrow/wellness-tourism-app"

          # ensure Space exists (Docker SDK)
          try:
              api.create_repo(repo_id=SPACE_ID, repo_type="space", space_sdk="docker", private=False)
          except Exception:
              pass

          files = [
              ("tourism_project/deployment/app.py", "app.py"),
              ("tourism_project/deployment/Dockerfile", "Dockerfile"),
              ("tourism_project/deployment/requirements.txt", "requirements.txt"),
          ]
          for local, remote in files:
              if not os.path.exists(local):
                  raise FileNotFoundError(f"Missing file: {local}")
              upload_file(
                  path_or_fileobj=local,
                  path_in_repo=remote,
                  repo_id=SPACE_ID,
                  repo_type="space",
                  token=os.environ["HF_TOKEN"]
              )
          print(f"✅ Space updated: https://huggingface.co/spaces/{SPACE_ID}")
          PY
